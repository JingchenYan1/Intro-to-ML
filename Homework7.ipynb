{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMJI00HZoeR9Ubu8hTjCJEP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JingchenYan1/Intro-to-ML/blob/main/Homework7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwTOgmuSoqbR",
        "outputId": "19235136-acf4-409f-8207-1b85c6ae6969"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Epoch [1/200], Loss: 2.1171, Test Acc: 31.88%\n",
            "Epoch [2/200], Loss: 1.8636, Test Acc: 36.64%\n",
            "Epoch [3/200], Loss: 1.7203, Test Acc: 41.67%\n",
            "Epoch [4/200], Loss: 1.6136, Test Acc: 45.96%\n",
            "Epoch [5/200], Loss: 1.5279, Test Acc: 47.43%\n",
            "Epoch [6/200], Loss: 1.4715, Test Acc: 49.69%\n",
            "Epoch [7/200], Loss: 1.4217, Test Acc: 52.02%\n",
            "Epoch [8/200], Loss: 1.3894, Test Acc: 52.67%\n",
            "Epoch [9/200], Loss: 1.3574, Test Acc: 54.55%\n",
            "Epoch [10/200], Loss: 1.3236, Test Acc: 54.79%\n",
            "Epoch [11/200], Loss: 1.2952, Test Acc: 57.39%\n",
            "Epoch [12/200], Loss: 1.2676, Test Acc: 58.71%\n",
            "Epoch [13/200], Loss: 1.2338, Test Acc: 59.59%\n",
            "Epoch [14/200], Loss: 1.2109, Test Acc: 60.42%\n",
            "Epoch [15/200], Loss: 1.1879, Test Acc: 60.33%\n",
            "Epoch [16/200], Loss: 1.1569, Test Acc: 62.61%\n",
            "Epoch [17/200], Loss: 1.1384, Test Acc: 63.50%\n",
            "Epoch [18/200], Loss: 1.1162, Test Acc: 63.76%\n",
            "Epoch [19/200], Loss: 1.0979, Test Acc: 64.61%\n",
            "Epoch [20/200], Loss: 1.0829, Test Acc: 64.51%\n",
            "Epoch [21/200], Loss: 1.0648, Test Acc: 64.90%\n",
            "Epoch [22/200], Loss: 1.0466, Test Acc: 65.75%\n",
            "Epoch [23/200], Loss: 1.0353, Test Acc: 65.90%\n",
            "Epoch [24/200], Loss: 1.0207, Test Acc: 66.77%\n",
            "Epoch [25/200], Loss: 1.0110, Test Acc: 67.26%\n",
            "Epoch [26/200], Loss: 0.9984, Test Acc: 67.80%\n",
            "Epoch [27/200], Loss: 0.9888, Test Acc: 67.66%\n",
            "Epoch [28/200], Loss: 0.9723, Test Acc: 68.14%\n",
            "Epoch [29/200], Loss: 0.9636, Test Acc: 69.11%\n",
            "Epoch [30/200], Loss: 0.9589, Test Acc: 69.02%\n",
            "Epoch [31/200], Loss: 0.9436, Test Acc: 69.17%\n",
            "Epoch [32/200], Loss: 0.9372, Test Acc: 69.86%\n",
            "Epoch [33/200], Loss: 0.9289, Test Acc: 70.19%\n",
            "Epoch [34/200], Loss: 0.9197, Test Acc: 69.06%\n",
            "Epoch [35/200], Loss: 0.9148, Test Acc: 70.62%\n",
            "Epoch [36/200], Loss: 0.9027, Test Acc: 70.81%\n",
            "Epoch [37/200], Loss: 0.8972, Test Acc: 70.49%\n",
            "Epoch [38/200], Loss: 0.8902, Test Acc: 71.05%\n",
            "Epoch [39/200], Loss: 0.8867, Test Acc: 70.98%\n",
            "Epoch [40/200], Loss: 0.8772, Test Acc: 71.55%\n",
            "Epoch [41/200], Loss: 0.8695, Test Acc: 71.43%\n",
            "Epoch [42/200], Loss: 0.8633, Test Acc: 71.36%\n",
            "Epoch [43/200], Loss: 0.8551, Test Acc: 70.74%\n",
            "Epoch [44/200], Loss: 0.8534, Test Acc: 72.27%\n",
            "Epoch [45/200], Loss: 0.8392, Test Acc: 72.53%\n",
            "Epoch [46/200], Loss: 0.8412, Test Acc: 72.17%\n",
            "Epoch [47/200], Loss: 0.8340, Test Acc: 72.16%\n",
            "Epoch [48/200], Loss: 0.8225, Test Acc: 72.66%\n",
            "Epoch [49/200], Loss: 0.8250, Test Acc: 73.39%\n",
            "Epoch [50/200], Loss: 0.8174, Test Acc: 73.57%\n",
            "Epoch [51/200], Loss: 0.8054, Test Acc: 73.12%\n",
            "Epoch [52/200], Loss: 0.8050, Test Acc: 73.20%\n",
            "Epoch [53/200], Loss: 0.8016, Test Acc: 73.59%\n",
            "Epoch [54/200], Loss: 0.7973, Test Acc: 73.08%\n",
            "Epoch [55/200], Loss: 0.7898, Test Acc: 73.37%\n",
            "Epoch [56/200], Loss: 0.7888, Test Acc: 73.04%\n",
            "Epoch [57/200], Loss: 0.7853, Test Acc: 73.70%\n",
            "Epoch [58/200], Loss: 0.7754, Test Acc: 74.28%\n",
            "Epoch [59/200], Loss: 0.7760, Test Acc: 73.89%\n",
            "Epoch [60/200], Loss: 0.7653, Test Acc: 73.91%\n",
            "Epoch [61/200], Loss: 0.7702, Test Acc: 74.17%\n",
            "Epoch [62/200], Loss: 0.7666, Test Acc: 74.56%\n",
            "Epoch [63/200], Loss: 0.7590, Test Acc: 74.24%\n",
            "Epoch [64/200], Loss: 0.7541, Test Acc: 74.70%\n",
            "Epoch [65/200], Loss: 0.7477, Test Acc: 74.98%\n",
            "Epoch [66/200], Loss: 0.7462, Test Acc: 74.74%\n",
            "Epoch [67/200], Loss: 0.7416, Test Acc: 74.42%\n",
            "Epoch [68/200], Loss: 0.7436, Test Acc: 74.77%\n",
            "Epoch [69/200], Loss: 0.7369, Test Acc: 74.58%\n",
            "Epoch [70/200], Loss: 0.7311, Test Acc: 74.99%\n",
            "Epoch [71/200], Loss: 0.7316, Test Acc: 75.06%\n",
            "Epoch [72/200], Loss: 0.7292, Test Acc: 74.82%\n",
            "Epoch [73/200], Loss: 0.7254, Test Acc: 75.59%\n",
            "Epoch [74/200], Loss: 0.7219, Test Acc: 74.80%\n",
            "Epoch [75/200], Loss: 0.7165, Test Acc: 75.69%\n",
            "Epoch [76/200], Loss: 0.7162, Test Acc: 75.74%\n",
            "Epoch [77/200], Loss: 0.7088, Test Acc: 75.54%\n",
            "Epoch [78/200], Loss: 0.7107, Test Acc: 75.77%\n",
            "Epoch [79/200], Loss: 0.7073, Test Acc: 75.47%\n",
            "Epoch [80/200], Loss: 0.7009, Test Acc: 75.79%\n",
            "Epoch [81/200], Loss: 0.7031, Test Acc: 75.75%\n",
            "Epoch [82/200], Loss: 0.6956, Test Acc: 76.17%\n",
            "Epoch [83/200], Loss: 0.6957, Test Acc: 75.87%\n",
            "Epoch [84/200], Loss: 0.6920, Test Acc: 75.61%\n",
            "Epoch [85/200], Loss: 0.6949, Test Acc: 75.73%\n",
            "Epoch [86/200], Loss: 0.6866, Test Acc: 75.53%\n",
            "Epoch [87/200], Loss: 0.6900, Test Acc: 75.97%\n",
            "Epoch [88/200], Loss: 0.6840, Test Acc: 75.29%\n",
            "Epoch [89/200], Loss: 0.6814, Test Acc: 75.77%\n",
            "Epoch [90/200], Loss: 0.6798, Test Acc: 75.84%\n",
            "Epoch [91/200], Loss: 0.6762, Test Acc: 76.02%\n",
            "Epoch [92/200], Loss: 0.6722, Test Acc: 76.46%\n",
            "Epoch [93/200], Loss: 0.6750, Test Acc: 76.21%\n",
            "Epoch [94/200], Loss: 0.6726, Test Acc: 76.27%\n",
            "Epoch [95/200], Loss: 0.6647, Test Acc: 75.61%\n",
            "Epoch [96/200], Loss: 0.6604, Test Acc: 76.27%\n",
            "Epoch [97/200], Loss: 0.6628, Test Acc: 76.12%\n",
            "Epoch [98/200], Loss: 0.6554, Test Acc: 76.89%\n",
            "Epoch [99/200], Loss: 0.6610, Test Acc: 75.91%\n",
            "Epoch [100/200], Loss: 0.6583, Test Acc: 76.61%\n",
            "Epoch [101/200], Loss: 0.6594, Test Acc: 76.77%\n",
            "Epoch [102/200], Loss: 0.6544, Test Acc: 76.66%\n",
            "Epoch [103/200], Loss: 0.6552, Test Acc: 77.01%\n",
            "Epoch [104/200], Loss: 0.6491, Test Acc: 77.01%\n",
            "Epoch [105/200], Loss: 0.6472, Test Acc: 75.80%\n",
            "Epoch [106/200], Loss: 0.6463, Test Acc: 77.01%\n",
            "Epoch [107/200], Loss: 0.6409, Test Acc: 76.61%\n",
            "Epoch [108/200], Loss: 0.6416, Test Acc: 76.73%\n",
            "Epoch [109/200], Loss: 0.6390, Test Acc: 77.09%\n",
            "Epoch [110/200], Loss: 0.6413, Test Acc: 76.74%\n",
            "Epoch [111/200], Loss: 0.6410, Test Acc: 77.35%\n",
            "Epoch [112/200], Loss: 0.6349, Test Acc: 77.33%\n",
            "Epoch [113/200], Loss: 0.6374, Test Acc: 77.41%\n",
            "Epoch [114/200], Loss: 0.6330, Test Acc: 77.17%\n",
            "Epoch [115/200], Loss: 0.6364, Test Acc: 77.17%\n",
            "Epoch [116/200], Loss: 0.6293, Test Acc: 76.57%\n",
            "Epoch [117/200], Loss: 0.6268, Test Acc: 77.53%\n",
            "Epoch [118/200], Loss: 0.6278, Test Acc: 77.41%\n",
            "Epoch [119/200], Loss: 0.6222, Test Acc: 77.59%\n",
            "Epoch [120/200], Loss: 0.6211, Test Acc: 77.53%\n",
            "Epoch [121/200], Loss: 0.6193, Test Acc: 76.95%\n",
            "Epoch [122/200], Loss: 0.6180, Test Acc: 77.25%\n",
            "Epoch [123/200], Loss: 0.6196, Test Acc: 77.76%\n",
            "Epoch [124/200], Loss: 0.6128, Test Acc: 77.50%\n",
            "Epoch [125/200], Loss: 0.6147, Test Acc: 77.31%\n",
            "Epoch [126/200], Loss: 0.6115, Test Acc: 77.62%\n",
            "Epoch [127/200], Loss: 0.6089, Test Acc: 77.56%\n",
            "Epoch [128/200], Loss: 0.6103, Test Acc: 77.33%\n",
            "Epoch [129/200], Loss: 0.6093, Test Acc: 77.04%\n",
            "Epoch [130/200], Loss: 0.6076, Test Acc: 77.80%\n",
            "Epoch [131/200], Loss: 0.6082, Test Acc: 78.07%\n",
            "Epoch [132/200], Loss: 0.6030, Test Acc: 77.59%\n",
            "Epoch [133/200], Loss: 0.5996, Test Acc: 77.13%\n",
            "Epoch [134/200], Loss: 0.6033, Test Acc: 77.46%\n",
            "Epoch [135/200], Loss: 0.6003, Test Acc: 77.99%\n",
            "Epoch [136/200], Loss: 0.5959, Test Acc: 77.68%\n",
            "Epoch [137/200], Loss: 0.5975, Test Acc: 77.95%\n",
            "Epoch [138/200], Loss: 0.5931, Test Acc: 78.01%\n",
            "Epoch [139/200], Loss: 0.5943, Test Acc: 77.94%\n",
            "Epoch [140/200], Loss: 0.5960, Test Acc: 78.34%\n",
            "Epoch [141/200], Loss: 0.5940, Test Acc: 77.86%\n",
            "Epoch [142/200], Loss: 0.5883, Test Acc: 78.01%\n",
            "Epoch [143/200], Loss: 0.5902, Test Acc: 77.66%\n",
            "Epoch [144/200], Loss: 0.5847, Test Acc: 77.57%\n",
            "Epoch [145/200], Loss: 0.5844, Test Acc: 78.00%\n",
            "Epoch [146/200], Loss: 0.5866, Test Acc: 78.23%\n",
            "Epoch [147/200], Loss: 0.5844, Test Acc: 78.23%\n",
            "Epoch [148/200], Loss: 0.5844, Test Acc: 78.12%\n",
            "Epoch [149/200], Loss: 0.5849, Test Acc: 78.36%\n",
            "Epoch [150/200], Loss: 0.5801, Test Acc: 78.09%\n",
            "Epoch [151/200], Loss: 0.5768, Test Acc: 77.93%\n",
            "Epoch [152/200], Loss: 0.5766, Test Acc: 78.12%\n",
            "Epoch [153/200], Loss: 0.5793, Test Acc: 77.92%\n",
            "Epoch [154/200], Loss: 0.5718, Test Acc: 78.22%\n",
            "Epoch [155/200], Loss: 0.5738, Test Acc: 77.96%\n",
            "Epoch [156/200], Loss: 0.5768, Test Acc: 77.90%\n",
            "Epoch [157/200], Loss: 0.5733, Test Acc: 78.38%\n",
            "Epoch [158/200], Loss: 0.5669, Test Acc: 78.48%\n",
            "Epoch [159/200], Loss: 0.5705, Test Acc: 77.21%\n",
            "Epoch [160/200], Loss: 0.5693, Test Acc: 77.90%\n",
            "Epoch [161/200], Loss: 0.5688, Test Acc: 78.31%\n",
            "Epoch [162/200], Loss: 0.5691, Test Acc: 78.10%\n",
            "Epoch [163/200], Loss: 0.5622, Test Acc: 77.96%\n",
            "Epoch [164/200], Loss: 0.5640, Test Acc: 78.05%\n",
            "Epoch [165/200], Loss: 0.5662, Test Acc: 78.44%\n",
            "Epoch [166/200], Loss: 0.5624, Test Acc: 78.54%\n",
            "Epoch [167/200], Loss: 0.5587, Test Acc: 78.46%\n",
            "Epoch [168/200], Loss: 0.5577, Test Acc: 78.61%\n",
            "Epoch [169/200], Loss: 0.5573, Test Acc: 78.91%\n",
            "Epoch [170/200], Loss: 0.5571, Test Acc: 78.24%\n",
            "Epoch [171/200], Loss: 0.5538, Test Acc: 78.70%\n",
            "Epoch [172/200], Loss: 0.5563, Test Acc: 78.33%\n",
            "Epoch [173/200], Loss: 0.5516, Test Acc: 78.42%\n",
            "Epoch [174/200], Loss: 0.5559, Test Acc: 78.54%\n",
            "Epoch [175/200], Loss: 0.5532, Test Acc: 78.13%\n",
            "Epoch [176/200], Loss: 0.5519, Test Acc: 78.43%\n",
            "Epoch [177/200], Loss: 0.5475, Test Acc: 78.74%\n",
            "Epoch [178/200], Loss: 0.5521, Test Acc: 78.52%\n",
            "Epoch [179/200], Loss: 0.5519, Test Acc: 78.24%\n",
            "Epoch [180/200], Loss: 0.5465, Test Acc: 78.65%\n",
            "Epoch [181/200], Loss: 0.5465, Test Acc: 78.81%\n",
            "Epoch [182/200], Loss: 0.5457, Test Acc: 78.30%\n",
            "Epoch [183/200], Loss: 0.5460, Test Acc: 78.96%\n",
            "Epoch [184/200], Loss: 0.5436, Test Acc: 78.56%\n",
            "Epoch [185/200], Loss: 0.5449, Test Acc: 78.84%\n",
            "Epoch [186/200], Loss: 0.5466, Test Acc: 78.48%\n",
            "Epoch [187/200], Loss: 0.5405, Test Acc: 78.55%\n",
            "Epoch [188/200], Loss: 0.5405, Test Acc: 77.82%\n",
            "Epoch [189/200], Loss: 0.5413, Test Acc: 78.74%\n",
            "Epoch [190/200], Loss: 0.5409, Test Acc: 78.62%\n",
            "Epoch [191/200], Loss: 0.5439, Test Acc: 78.47%\n",
            "Epoch [192/200], Loss: 0.5405, Test Acc: 78.89%\n",
            "Epoch [193/200], Loss: 0.5401, Test Acc: 78.87%\n",
            "Epoch [194/200], Loss: 0.5358, Test Acc: 78.39%\n",
            "Epoch [195/200], Loss: 0.5407, Test Acc: 78.45%\n",
            "Epoch [196/200], Loss: 0.5364, Test Acc: 79.23%\n",
            "Epoch [197/200], Loss: 0.5343, Test Acc: 78.90%\n",
            "Epoch [198/200], Loss: 0.5316, Test Acc: 78.59%\n",
            "Epoch [199/200], Loss: 0.5306, Test Acc: 78.66%\n",
            "Epoch [200/200], Loss: 0.5272, Test Acc: 78.84%\n",
            "Training time: 2454.82s after 200 epochs\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))  # [N,16,16,16]\n",
        "        x = self.pool(torch.relu(self.conv2(x)))  # [N,32,8,8]\n",
        "        x = x.view(-1, 32 * 8 * 8)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "def train(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    return epoch_loss\n",
        "\n",
        "def test(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    acc = 100. * correct / total\n",
        "    return acc\n",
        "\n",
        "num_epochs = 200\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, trainloader, optimizer, criterion, device)\n",
        "    acc = test(model, testloader, device)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Test Acc: {acc:.2f}%\")\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print(f\"Training time: {training_time:.2f}s after {num_epochs} epochs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "class DeeperCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeeperCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # 新增一层卷积\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = self.pool(torch.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 64 * 4 * 4)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = DeeperCNN().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "def train(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()*inputs.size(0)\n",
        "    epoch_loss = running_loss/len(loader.dataset)\n",
        "    return epoch_loss\n",
        "\n",
        "def test(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    acc = 100.*correct/total\n",
        "    return acc\n",
        "\n",
        "num_epochs = 200\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, trainloader, optimizer, criterion, device)\n",
        "    test_acc = test(model, testloader, device)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {train_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g88ctZn3w-fe",
        "outputId": "e58e647d-5da3-492a-c604-1e568a988799"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/200] Loss: 2.2719, Test Acc: 23.65%\n",
            "Epoch [2/200] Loss: 2.0543, Test Acc: 29.89%\n",
            "Epoch [3/200] Loss: 1.9233, Test Acc: 34.43%\n",
            "Epoch [4/200] Loss: 1.8155, Test Acc: 37.96%\n",
            "Epoch [5/200] Loss: 1.7106, Test Acc: 41.44%\n",
            "Epoch [6/200] Loss: 1.6264, Test Acc: 45.39%\n",
            "Epoch [7/200] Loss: 1.5519, Test Acc: 47.99%\n",
            "Epoch [8/200] Loss: 1.4896, Test Acc: 50.06%\n",
            "Epoch [9/200] Loss: 1.4444, Test Acc: 52.78%\n",
            "Epoch [10/200] Loss: 1.4097, Test Acc: 53.98%\n",
            "Epoch [11/200] Loss: 1.3708, Test Acc: 54.06%\n",
            "Epoch [12/200] Loss: 1.3400, Test Acc: 56.69%\n",
            "Epoch [13/200] Loss: 1.3101, Test Acc: 56.72%\n",
            "Epoch [14/200] Loss: 1.2830, Test Acc: 58.21%\n",
            "Epoch [15/200] Loss: 1.2479, Test Acc: 59.41%\n",
            "Epoch [16/200] Loss: 1.2273, Test Acc: 59.23%\n",
            "Epoch [17/200] Loss: 1.1989, Test Acc: 60.96%\n",
            "Epoch [18/200] Loss: 1.1773, Test Acc: 60.68%\n",
            "Epoch [19/200] Loss: 1.1601, Test Acc: 62.04%\n",
            "Epoch [20/200] Loss: 1.1368, Test Acc: 61.54%\n",
            "Epoch [21/200] Loss: 1.1178, Test Acc: 63.22%\n",
            "Epoch [22/200] Loss: 1.0962, Test Acc: 63.95%\n",
            "Epoch [23/200] Loss: 1.0777, Test Acc: 64.80%\n",
            "Epoch [24/200] Loss: 1.0663, Test Acc: 64.05%\n",
            "Epoch [25/200] Loss: 1.0448, Test Acc: 64.51%\n",
            "Epoch [26/200] Loss: 1.0304, Test Acc: 65.18%\n",
            "Epoch [27/200] Loss: 1.0154, Test Acc: 66.54%\n",
            "Epoch [28/200] Loss: 1.0034, Test Acc: 67.51%\n",
            "Epoch [29/200] Loss: 0.9898, Test Acc: 67.23%\n",
            "Epoch [30/200] Loss: 0.9775, Test Acc: 68.11%\n",
            "Epoch [31/200] Loss: 0.9681, Test Acc: 67.99%\n",
            "Epoch [32/200] Loss: 0.9529, Test Acc: 67.72%\n",
            "Epoch [33/200] Loss: 0.9423, Test Acc: 69.46%\n",
            "Epoch [34/200] Loss: 0.9361, Test Acc: 68.71%\n",
            "Epoch [35/200] Loss: 0.9183, Test Acc: 68.99%\n",
            "Epoch [36/200] Loss: 0.9114, Test Acc: 69.13%\n",
            "Epoch [37/200] Loss: 0.8920, Test Acc: 69.74%\n",
            "Epoch [38/200] Loss: 0.8924, Test Acc: 69.59%\n",
            "Epoch [39/200] Loss: 0.8821, Test Acc: 69.57%\n",
            "Epoch [40/200] Loss: 0.8732, Test Acc: 71.17%\n",
            "Epoch [41/200] Loss: 0.8599, Test Acc: 71.57%\n",
            "Epoch [42/200] Loss: 0.8559, Test Acc: 70.39%\n",
            "Epoch [43/200] Loss: 0.8497, Test Acc: 71.32%\n",
            "Epoch [44/200] Loss: 0.8429, Test Acc: 71.41%\n",
            "Epoch [45/200] Loss: 0.8322, Test Acc: 70.44%\n",
            "Epoch [46/200] Loss: 0.8222, Test Acc: 72.51%\n",
            "Epoch [47/200] Loss: 0.8194, Test Acc: 72.83%\n",
            "Epoch [48/200] Loss: 0.8070, Test Acc: 72.14%\n",
            "Epoch [49/200] Loss: 0.7997, Test Acc: 73.12%\n",
            "Epoch [50/200] Loss: 0.7952, Test Acc: 73.26%\n",
            "Epoch [51/200] Loss: 0.7896, Test Acc: 73.13%\n",
            "Epoch [52/200] Loss: 0.7846, Test Acc: 73.50%\n",
            "Epoch [53/200] Loss: 0.7748, Test Acc: 73.66%\n",
            "Epoch [54/200] Loss: 0.7679, Test Acc: 73.38%\n",
            "Epoch [55/200] Loss: 0.7632, Test Acc: 74.22%\n",
            "Epoch [56/200] Loss: 0.7574, Test Acc: 74.57%\n",
            "Epoch [57/200] Loss: 0.7496, Test Acc: 74.74%\n",
            "Epoch [58/200] Loss: 0.7527, Test Acc: 74.39%\n",
            "Epoch [59/200] Loss: 0.7419, Test Acc: 74.71%\n",
            "Epoch [60/200] Loss: 0.7348, Test Acc: 74.41%\n",
            "Epoch [61/200] Loss: 0.7286, Test Acc: 74.76%\n",
            "Epoch [62/200] Loss: 0.7223, Test Acc: 74.77%\n",
            "Epoch [63/200] Loss: 0.7244, Test Acc: 74.33%\n",
            "Epoch [64/200] Loss: 0.7194, Test Acc: 74.95%\n",
            "Epoch [65/200] Loss: 0.7139, Test Acc: 74.74%\n",
            "Epoch [66/200] Loss: 0.7071, Test Acc: 75.60%\n",
            "Epoch [67/200] Loss: 0.7044, Test Acc: 75.18%\n",
            "Epoch [68/200] Loss: 0.6959, Test Acc: 75.51%\n",
            "Epoch [69/200] Loss: 0.6938, Test Acc: 74.32%\n",
            "Epoch [70/200] Loss: 0.6908, Test Acc: 75.58%\n",
            "Epoch [71/200] Loss: 0.6803, Test Acc: 76.42%\n",
            "Epoch [72/200] Loss: 0.6771, Test Acc: 75.66%\n",
            "Epoch [73/200] Loss: 0.6750, Test Acc: 76.19%\n",
            "Epoch [74/200] Loss: 0.6734, Test Acc: 76.02%\n",
            "Epoch [75/200] Loss: 0.6696, Test Acc: 76.27%\n",
            "Epoch [76/200] Loss: 0.6681, Test Acc: 76.91%\n",
            "Epoch [77/200] Loss: 0.6675, Test Acc: 76.77%\n",
            "Epoch [78/200] Loss: 0.6585, Test Acc: 76.86%\n",
            "Epoch [79/200] Loss: 0.6504, Test Acc: 76.48%\n",
            "Epoch [80/200] Loss: 0.6522, Test Acc: 77.23%\n",
            "Epoch [81/200] Loss: 0.6487, Test Acc: 76.87%\n",
            "Epoch [82/200] Loss: 0.6440, Test Acc: 77.26%\n",
            "Epoch [83/200] Loss: 0.6459, Test Acc: 77.13%\n",
            "Epoch [84/200] Loss: 0.6422, Test Acc: 77.06%\n",
            "Epoch [85/200] Loss: 0.6342, Test Acc: 77.56%\n",
            "Epoch [86/200] Loss: 0.6306, Test Acc: 77.46%\n",
            "Epoch [87/200] Loss: 0.6287, Test Acc: 76.89%\n",
            "Epoch [88/200] Loss: 0.6289, Test Acc: 77.86%\n",
            "Epoch [89/200] Loss: 0.6240, Test Acc: 77.63%\n",
            "Epoch [90/200] Loss: 0.6237, Test Acc: 77.60%\n",
            "Epoch [91/200] Loss: 0.6180, Test Acc: 77.91%\n",
            "Epoch [92/200] Loss: 0.6119, Test Acc: 77.64%\n",
            "Epoch [93/200] Loss: 0.6134, Test Acc: 77.87%\n",
            "Epoch [94/200] Loss: 0.6113, Test Acc: 77.94%\n",
            "Epoch [95/200] Loss: 0.6066, Test Acc: 77.87%\n",
            "Epoch [96/200] Loss: 0.6060, Test Acc: 78.50%\n",
            "Epoch [97/200] Loss: 0.6068, Test Acc: 78.09%\n",
            "Epoch [98/200] Loss: 0.6069, Test Acc: 78.45%\n",
            "Epoch [99/200] Loss: 0.5988, Test Acc: 78.29%\n",
            "Epoch [100/200] Loss: 0.5947, Test Acc: 77.87%\n",
            "Epoch [101/200] Loss: 0.5932, Test Acc: 78.68%\n",
            "Epoch [102/200] Loss: 0.5891, Test Acc: 78.48%\n",
            "Epoch [103/200] Loss: 0.5924, Test Acc: 78.69%\n",
            "Epoch [104/200] Loss: 0.5846, Test Acc: 78.34%\n",
            "Epoch [105/200] Loss: 0.5811, Test Acc: 78.69%\n",
            "Epoch [106/200] Loss: 0.5803, Test Acc: 78.54%\n",
            "Epoch [107/200] Loss: 0.5802, Test Acc: 79.03%\n",
            "Epoch [108/200] Loss: 0.5745, Test Acc: 78.48%\n",
            "Epoch [109/200] Loss: 0.5756, Test Acc: 79.09%\n",
            "Epoch [110/200] Loss: 0.5693, Test Acc: 78.28%\n",
            "Epoch [111/200] Loss: 0.5743, Test Acc: 78.46%\n",
            "Epoch [112/200] Loss: 0.5682, Test Acc: 78.48%\n",
            "Epoch [113/200] Loss: 0.5662, Test Acc: 79.13%\n",
            "Epoch [114/200] Loss: 0.5643, Test Acc: 78.83%\n",
            "Epoch [115/200] Loss: 0.5593, Test Acc: 79.18%\n",
            "Epoch [116/200] Loss: 0.5589, Test Acc: 78.81%\n",
            "Epoch [117/200] Loss: 0.5593, Test Acc: 78.40%\n",
            "Epoch [118/200] Loss: 0.5563, Test Acc: 78.42%\n",
            "Epoch [119/200] Loss: 0.5523, Test Acc: 79.27%\n",
            "Epoch [120/200] Loss: 0.5536, Test Acc: 79.28%\n",
            "Epoch [121/200] Loss: 0.5491, Test Acc: 78.76%\n",
            "Epoch [122/200] Loss: 0.5469, Test Acc: 79.08%\n",
            "Epoch [123/200] Loss: 0.5478, Test Acc: 78.94%\n",
            "Epoch [124/200] Loss: 0.5463, Test Acc: 79.45%\n",
            "Epoch [125/200] Loss: 0.5408, Test Acc: 79.09%\n",
            "Epoch [126/200] Loss: 0.5391, Test Acc: 79.41%\n",
            "Epoch [127/200] Loss: 0.5392, Test Acc: 79.74%\n",
            "Epoch [128/200] Loss: 0.5371, Test Acc: 79.10%\n",
            "Epoch [129/200] Loss: 0.5372, Test Acc: 79.05%\n",
            "Epoch [130/200] Loss: 0.5340, Test Acc: 79.70%\n",
            "Epoch [131/200] Loss: 0.5335, Test Acc: 78.25%\n",
            "Epoch [132/200] Loss: 0.5344, Test Acc: 79.51%\n",
            "Epoch [133/200] Loss: 0.5276, Test Acc: 79.25%\n",
            "Epoch [134/200] Loss: 0.5252, Test Acc: 79.29%\n",
            "Epoch [135/200] Loss: 0.5286, Test Acc: 79.86%\n",
            "Epoch [136/200] Loss: 0.5217, Test Acc: 79.40%\n",
            "Epoch [137/200] Loss: 0.5208, Test Acc: 79.62%\n",
            "Epoch [138/200] Loss: 0.5199, Test Acc: 80.01%\n",
            "Epoch [139/200] Loss: 0.5158, Test Acc: 79.93%\n",
            "Epoch [140/200] Loss: 0.5188, Test Acc: 79.69%\n",
            "Epoch [141/200] Loss: 0.5157, Test Acc: 79.77%\n",
            "Epoch [142/200] Loss: 0.5135, Test Acc: 80.14%\n",
            "Epoch [143/200] Loss: 0.5148, Test Acc: 80.02%\n",
            "Epoch [144/200] Loss: 0.5096, Test Acc: 79.91%\n",
            "Epoch [145/200] Loss: 0.5069, Test Acc: 79.71%\n",
            "Epoch [146/200] Loss: 0.5066, Test Acc: 80.16%\n",
            "Epoch [147/200] Loss: 0.5063, Test Acc: 79.70%\n",
            "Epoch [148/200] Loss: 0.5068, Test Acc: 80.14%\n",
            "Epoch [149/200] Loss: 0.5008, Test Acc: 79.75%\n",
            "Epoch [150/200] Loss: 0.4977, Test Acc: 79.42%\n",
            "Epoch [151/200] Loss: 0.4984, Test Acc: 80.71%\n",
            "Epoch [152/200] Loss: 0.5006, Test Acc: 79.78%\n",
            "Epoch [153/200] Loss: 0.4982, Test Acc: 80.17%\n",
            "Epoch [154/200] Loss: 0.4975, Test Acc: 79.91%\n",
            "Epoch [155/200] Loss: 0.4920, Test Acc: 79.17%\n",
            "Epoch [156/200] Loss: 0.4981, Test Acc: 79.57%\n",
            "Epoch [157/200] Loss: 0.4949, Test Acc: 79.36%\n",
            "Epoch [158/200] Loss: 0.4917, Test Acc: 79.93%\n",
            "Epoch [159/200] Loss: 0.4950, Test Acc: 80.21%\n",
            "Epoch [160/200] Loss: 0.4904, Test Acc: 79.94%\n",
            "Epoch [161/200] Loss: 0.4865, Test Acc: 80.01%\n",
            "Epoch [162/200] Loss: 0.4866, Test Acc: 79.82%\n",
            "Epoch [163/200] Loss: 0.4854, Test Acc: 80.35%\n",
            "Epoch [164/200] Loss: 0.4784, Test Acc: 80.23%\n",
            "Epoch [165/200] Loss: 0.4806, Test Acc: 80.12%\n",
            "Epoch [166/200] Loss: 0.4830, Test Acc: 80.59%\n",
            "Epoch [167/200] Loss: 0.4770, Test Acc: 79.88%\n",
            "Epoch [168/200] Loss: 0.4788, Test Acc: 80.16%\n",
            "Epoch [169/200] Loss: 0.4768, Test Acc: 80.53%\n",
            "Epoch [170/200] Loss: 0.4780, Test Acc: 80.35%\n",
            "Epoch [171/200] Loss: 0.4720, Test Acc: 80.75%\n",
            "Epoch [172/200] Loss: 0.4750, Test Acc: 80.02%\n",
            "Epoch [173/200] Loss: 0.4708, Test Acc: 80.24%\n",
            "Epoch [174/200] Loss: 0.4692, Test Acc: 80.19%\n",
            "Epoch [175/200] Loss: 0.4706, Test Acc: 80.14%\n",
            "Epoch [176/200] Loss: 0.4685, Test Acc: 80.49%\n",
            "Epoch [177/200] Loss: 0.4648, Test Acc: 80.80%\n",
            "Epoch [178/200] Loss: 0.4693, Test Acc: 80.69%\n",
            "Epoch [179/200] Loss: 0.4665, Test Acc: 80.04%\n",
            "Epoch [180/200] Loss: 0.4651, Test Acc: 80.41%\n",
            "Epoch [181/200] Loss: 0.4600, Test Acc: 80.26%\n",
            "Epoch [182/200] Loss: 0.4607, Test Acc: 80.67%\n",
            "Epoch [183/200] Loss: 0.4567, Test Acc: 80.56%\n",
            "Epoch [184/200] Loss: 0.4595, Test Acc: 80.37%\n",
            "Epoch [185/200] Loss: 0.4591, Test Acc: 80.50%\n",
            "Epoch [186/200] Loss: 0.4617, Test Acc: 80.68%\n",
            "Epoch [187/200] Loss: 0.4531, Test Acc: 80.65%\n",
            "Epoch [188/200] Loss: 0.4558, Test Acc: 80.49%\n",
            "Epoch [189/200] Loss: 0.4564, Test Acc: 80.62%\n",
            "Epoch [190/200] Loss: 0.4498, Test Acc: 80.99%\n",
            "Epoch [191/200] Loss: 0.4498, Test Acc: 80.33%\n",
            "Epoch [192/200] Loss: 0.4463, Test Acc: 80.90%\n",
            "Epoch [193/200] Loss: 0.4449, Test Acc: 80.48%\n",
            "Epoch [194/200] Loss: 0.4475, Test Acc: 80.78%\n",
            "Epoch [195/200] Loss: 0.4483, Test Acc: 80.69%\n",
            "Epoch [196/200] Loss: 0.4474, Test Acc: 80.97%\n",
            "Epoch [197/200] Loss: 0.4466, Test Acc: 80.84%\n",
            "Epoch [198/200] Loss: 0.4480, Test Acc: 80.88%\n",
            "Epoch [199/200] Loss: 0.4441, Test Acc: 81.03%\n",
            "Epoch [200/200] Loss: 0.4432, Test Acc: 81.05%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = torch.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 128, num_blocks[3], stride=2)\n",
        "\n",
        "        self.linear = nn.Linear(128, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_block, stride):\n",
        "        layers = []\n",
        "        layers.append(block(self.in_planes, planes, stride))\n",
        "        self.in_planes = planes\n",
        "        for i in range(1, num_block):\n",
        "            layers.append(block(planes, planes))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        out = torch.mean(out, dim=[2,3])  # Global Average Pooling\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "model = ResNet(BasicBlock, [1,1,1,1], num_classes=10).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "num_epochs = 200\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, trainloader, optimizer, criterion, device)\n",
        "    acc = test(model, testloader, device)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Test Acc: {acc:.2f}%\")\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print(f\"ResNet-10 Training time: {training_time:.2f}s after {num_epochs} epochs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAWqp7duw_vO",
        "outputId": "f3e36645-2ad1-497f-c804-9acadbdfbbd8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/200], Loss: 1.9224, Test Acc: 38.20%\n",
            "Epoch [2/200], Loss: 1.6187, Test Acc: 42.93%\n",
            "Epoch [3/200], Loss: 1.4635, Test Acc: 49.48%\n",
            "Epoch [4/200], Loss: 1.3512, Test Acc: 52.47%\n",
            "Epoch [5/200], Loss: 1.2649, Test Acc: 51.65%\n",
            "Epoch [6/200], Loss: 1.1940, Test Acc: 57.89%\n",
            "Epoch [7/200], Loss: 1.1365, Test Acc: 59.76%\n",
            "Epoch [8/200], Loss: 1.0772, Test Acc: 59.65%\n",
            "Epoch [9/200], Loss: 1.0308, Test Acc: 62.15%\n",
            "Epoch [10/200], Loss: 0.9854, Test Acc: 66.41%\n",
            "Epoch [11/200], Loss: 0.9476, Test Acc: 66.48%\n",
            "Epoch [12/200], Loss: 0.9103, Test Acc: 67.33%\n",
            "Epoch [13/200], Loss: 0.8804, Test Acc: 66.73%\n",
            "Epoch [14/200], Loss: 0.8499, Test Acc: 69.14%\n",
            "Epoch [15/200], Loss: 0.8257, Test Acc: 70.59%\n",
            "Epoch [16/200], Loss: 0.8018, Test Acc: 69.37%\n",
            "Epoch [17/200], Loss: 0.7756, Test Acc: 72.07%\n",
            "Epoch [18/200], Loss: 0.7586, Test Acc: 71.08%\n",
            "Epoch [19/200], Loss: 0.7302, Test Acc: 72.42%\n",
            "Epoch [20/200], Loss: 0.7188, Test Acc: 72.54%\n",
            "Epoch [21/200], Loss: 0.7021, Test Acc: 75.04%\n",
            "Epoch [22/200], Loss: 0.6865, Test Acc: 73.03%\n",
            "Epoch [23/200], Loss: 0.6694, Test Acc: 74.18%\n",
            "Epoch [24/200], Loss: 0.6553, Test Acc: 75.77%\n",
            "Epoch [25/200], Loss: 0.6414, Test Acc: 75.17%\n",
            "Epoch [26/200], Loss: 0.6268, Test Acc: 77.58%\n",
            "Epoch [27/200], Loss: 0.6193, Test Acc: 77.83%\n",
            "Epoch [28/200], Loss: 0.6037, Test Acc: 77.91%\n",
            "Epoch [29/200], Loss: 0.5903, Test Acc: 78.08%\n",
            "Epoch [30/200], Loss: 0.5862, Test Acc: 77.59%\n",
            "Epoch [31/200], Loss: 0.5769, Test Acc: 78.45%\n",
            "Epoch [32/200], Loss: 0.5652, Test Acc: 78.28%\n",
            "Epoch [33/200], Loss: 0.5615, Test Acc: 78.66%\n",
            "Epoch [34/200], Loss: 0.5467, Test Acc: 79.48%\n",
            "Epoch [35/200], Loss: 0.5384, Test Acc: 79.58%\n",
            "Epoch [36/200], Loss: 0.5320, Test Acc: 79.82%\n",
            "Epoch [37/200], Loss: 0.5259, Test Acc: 78.92%\n",
            "Epoch [38/200], Loss: 0.5178, Test Acc: 79.65%\n",
            "Epoch [39/200], Loss: 0.5147, Test Acc: 80.62%\n",
            "Epoch [40/200], Loss: 0.5062, Test Acc: 80.38%\n",
            "Epoch [41/200], Loss: 0.4992, Test Acc: 81.09%\n",
            "Epoch [42/200], Loss: 0.4908, Test Acc: 80.80%\n",
            "Epoch [43/200], Loss: 0.4845, Test Acc: 81.16%\n",
            "Epoch [44/200], Loss: 0.4792, Test Acc: 81.91%\n",
            "Epoch [45/200], Loss: 0.4787, Test Acc: 81.48%\n",
            "Epoch [46/200], Loss: 0.4697, Test Acc: 81.01%\n",
            "Epoch [47/200], Loss: 0.4680, Test Acc: 82.20%\n",
            "Epoch [48/200], Loss: 0.4623, Test Acc: 80.10%\n",
            "Epoch [49/200], Loss: 0.4572, Test Acc: 81.16%\n",
            "Epoch [50/200], Loss: 0.4522, Test Acc: 82.07%\n",
            "Epoch [51/200], Loss: 0.4485, Test Acc: 81.29%\n",
            "Epoch [52/200], Loss: 0.4441, Test Acc: 81.63%\n",
            "Epoch [53/200], Loss: 0.4369, Test Acc: 82.80%\n",
            "Epoch [54/200], Loss: 0.4323, Test Acc: 82.11%\n",
            "Epoch [55/200], Loss: 0.4339, Test Acc: 82.30%\n",
            "Epoch [56/200], Loss: 0.4271, Test Acc: 83.12%\n",
            "Epoch [57/200], Loss: 0.4233, Test Acc: 83.24%\n",
            "Epoch [58/200], Loss: 0.4230, Test Acc: 81.98%\n",
            "Epoch [59/200], Loss: 0.4153, Test Acc: 82.70%\n",
            "Epoch [60/200], Loss: 0.4115, Test Acc: 82.68%\n",
            "Epoch [61/200], Loss: 0.4122, Test Acc: 83.21%\n",
            "Epoch [62/200], Loss: 0.4062, Test Acc: 83.24%\n",
            "Epoch [63/200], Loss: 0.4038, Test Acc: 83.28%\n",
            "Epoch [64/200], Loss: 0.3964, Test Acc: 82.70%\n",
            "Epoch [65/200], Loss: 0.3983, Test Acc: 83.40%\n",
            "Epoch [66/200], Loss: 0.3939, Test Acc: 83.69%\n",
            "Epoch [67/200], Loss: 0.3894, Test Acc: 83.12%\n",
            "Epoch [68/200], Loss: 0.3843, Test Acc: 83.53%\n",
            "Epoch [69/200], Loss: 0.3834, Test Acc: 83.08%\n",
            "Epoch [70/200], Loss: 0.3849, Test Acc: 83.70%\n",
            "Epoch [71/200], Loss: 0.3778, Test Acc: 83.09%\n",
            "Epoch [72/200], Loss: 0.3751, Test Acc: 83.63%\n",
            "Epoch [73/200], Loss: 0.3737, Test Acc: 83.52%\n",
            "Epoch [74/200], Loss: 0.3681, Test Acc: 84.34%\n",
            "Epoch [75/200], Loss: 0.3668, Test Acc: 84.28%\n",
            "Epoch [76/200], Loss: 0.3663, Test Acc: 83.33%\n",
            "Epoch [77/200], Loss: 0.3673, Test Acc: 83.65%\n",
            "Epoch [78/200], Loss: 0.3617, Test Acc: 84.26%\n",
            "Epoch [79/200], Loss: 0.3569, Test Acc: 84.24%\n",
            "Epoch [80/200], Loss: 0.3586, Test Acc: 84.08%\n",
            "Epoch [81/200], Loss: 0.3500, Test Acc: 84.24%\n",
            "Epoch [82/200], Loss: 0.3536, Test Acc: 83.97%\n",
            "Epoch [83/200], Loss: 0.3469, Test Acc: 84.21%\n",
            "Epoch [84/200], Loss: 0.3424, Test Acc: 84.48%\n",
            "Epoch [85/200], Loss: 0.3448, Test Acc: 84.71%\n",
            "Epoch [86/200], Loss: 0.3429, Test Acc: 83.60%\n",
            "Epoch [87/200], Loss: 0.3422, Test Acc: 84.69%\n",
            "Epoch [88/200], Loss: 0.3380, Test Acc: 84.23%\n",
            "Epoch [89/200], Loss: 0.3358, Test Acc: 84.89%\n",
            "Epoch [90/200], Loss: 0.3325, Test Acc: 83.25%\n",
            "Epoch [91/200], Loss: 0.3306, Test Acc: 84.54%\n",
            "Epoch [92/200], Loss: 0.3316, Test Acc: 85.08%\n",
            "Epoch [93/200], Loss: 0.3253, Test Acc: 84.25%\n",
            "Epoch [94/200], Loss: 0.3238, Test Acc: 84.19%\n",
            "Epoch [95/200], Loss: 0.3245, Test Acc: 84.30%\n",
            "Epoch [96/200], Loss: 0.3232, Test Acc: 85.09%\n",
            "Epoch [97/200], Loss: 0.3144, Test Acc: 85.18%\n",
            "Epoch [98/200], Loss: 0.3186, Test Acc: 85.31%\n",
            "Epoch [99/200], Loss: 0.3155, Test Acc: 85.27%\n",
            "Epoch [100/200], Loss: 0.3153, Test Acc: 85.91%\n",
            "Epoch [101/200], Loss: 0.3121, Test Acc: 84.35%\n",
            "Epoch [102/200], Loss: 0.3119, Test Acc: 84.69%\n",
            "Epoch [103/200], Loss: 0.3084, Test Acc: 85.61%\n",
            "Epoch [104/200], Loss: 0.3069, Test Acc: 84.88%\n",
            "Epoch [105/200], Loss: 0.3032, Test Acc: 84.90%\n",
            "Epoch [106/200], Loss: 0.3074, Test Acc: 85.60%\n",
            "Epoch [107/200], Loss: 0.3097, Test Acc: 84.81%\n",
            "Epoch [108/200], Loss: 0.3054, Test Acc: 85.19%\n",
            "Epoch [109/200], Loss: 0.2968, Test Acc: 85.30%\n",
            "Epoch [110/200], Loss: 0.2972, Test Acc: 84.85%\n",
            "Epoch [111/200], Loss: 0.2950, Test Acc: 85.64%\n",
            "Epoch [112/200], Loss: 0.2923, Test Acc: 85.46%\n",
            "Epoch [113/200], Loss: 0.2905, Test Acc: 85.75%\n",
            "Epoch [114/200], Loss: 0.2887, Test Acc: 84.55%\n",
            "Epoch [115/200], Loss: 0.2889, Test Acc: 84.78%\n",
            "Epoch [116/200], Loss: 0.2903, Test Acc: 85.16%\n",
            "Epoch [117/200], Loss: 0.2874, Test Acc: 85.72%\n",
            "Epoch [118/200], Loss: 0.2829, Test Acc: 85.50%\n",
            "Epoch [119/200], Loss: 0.2856, Test Acc: 85.60%\n",
            "Epoch [120/200], Loss: 0.2826, Test Acc: 85.16%\n",
            "Epoch [121/200], Loss: 0.2830, Test Acc: 85.80%\n",
            "Epoch [122/200], Loss: 0.2792, Test Acc: 85.79%\n",
            "Epoch [123/200], Loss: 0.2786, Test Acc: 85.10%\n",
            "Epoch [124/200], Loss: 0.2786, Test Acc: 85.85%\n",
            "Epoch [125/200], Loss: 0.2694, Test Acc: 85.72%\n",
            "Epoch [126/200], Loss: 0.2733, Test Acc: 85.85%\n",
            "Epoch [127/200], Loss: 0.2737, Test Acc: 85.49%\n",
            "Epoch [128/200], Loss: 0.2690, Test Acc: 86.13%\n",
            "Epoch [129/200], Loss: 0.2665, Test Acc: 86.24%\n",
            "Epoch [130/200], Loss: 0.2674, Test Acc: 85.35%\n",
            "Epoch [131/200], Loss: 0.2672, Test Acc: 85.92%\n",
            "Epoch [132/200], Loss: 0.2666, Test Acc: 85.61%\n",
            "Epoch [133/200], Loss: 0.2675, Test Acc: 84.70%\n",
            "Epoch [134/200], Loss: 0.2649, Test Acc: 85.42%\n",
            "Epoch [135/200], Loss: 0.2640, Test Acc: 86.12%\n",
            "Epoch [136/200], Loss: 0.2614, Test Acc: 86.06%\n",
            "Epoch [137/200], Loss: 0.2594, Test Acc: 86.35%\n",
            "Epoch [138/200], Loss: 0.2628, Test Acc: 85.54%\n",
            "Epoch [139/200], Loss: 0.2594, Test Acc: 86.14%\n",
            "Epoch [140/200], Loss: 0.2572, Test Acc: 86.17%\n",
            "Epoch [141/200], Loss: 0.2559, Test Acc: 85.81%\n",
            "Epoch [142/200], Loss: 0.2520, Test Acc: 85.83%\n",
            "Epoch [143/200], Loss: 0.2498, Test Acc: 86.31%\n",
            "Epoch [144/200], Loss: 0.2512, Test Acc: 86.02%\n",
            "Epoch [145/200], Loss: 0.2477, Test Acc: 85.36%\n",
            "Epoch [146/200], Loss: 0.2517, Test Acc: 85.31%\n",
            "Epoch [147/200], Loss: 0.2480, Test Acc: 85.78%\n",
            "Epoch [148/200], Loss: 0.2460, Test Acc: 86.12%\n",
            "Epoch [149/200], Loss: 0.2448, Test Acc: 86.12%\n",
            "Epoch [150/200], Loss: 0.2416, Test Acc: 85.59%\n",
            "Epoch [151/200], Loss: 0.2448, Test Acc: 86.24%\n",
            "Epoch [152/200], Loss: 0.2430, Test Acc: 85.85%\n",
            "Epoch [153/200], Loss: 0.2419, Test Acc: 86.20%\n",
            "Epoch [154/200], Loss: 0.2423, Test Acc: 85.94%\n",
            "Epoch [155/200], Loss: 0.2351, Test Acc: 85.77%\n",
            "Epoch [156/200], Loss: 0.2399, Test Acc: 85.91%\n",
            "Epoch [157/200], Loss: 0.2381, Test Acc: 86.12%\n",
            "Epoch [158/200], Loss: 0.2377, Test Acc: 86.12%\n",
            "Epoch [159/200], Loss: 0.2354, Test Acc: 85.91%\n",
            "Epoch [160/200], Loss: 0.2306, Test Acc: 85.67%\n",
            "Epoch [161/200], Loss: 0.2312, Test Acc: 86.03%\n",
            "Epoch [162/200], Loss: 0.2284, Test Acc: 85.99%\n",
            "Epoch [163/200], Loss: 0.2328, Test Acc: 85.76%\n",
            "Epoch [164/200], Loss: 0.2328, Test Acc: 86.10%\n",
            "Epoch [165/200], Loss: 0.2266, Test Acc: 85.41%\n",
            "Epoch [166/200], Loss: 0.2292, Test Acc: 86.06%\n",
            "Epoch [167/200], Loss: 0.2292, Test Acc: 85.86%\n",
            "Epoch [168/200], Loss: 0.2242, Test Acc: 86.39%\n",
            "Epoch [169/200], Loss: 0.2279, Test Acc: 86.34%\n",
            "Epoch [170/200], Loss: 0.2242, Test Acc: 86.41%\n",
            "Epoch [171/200], Loss: 0.2252, Test Acc: 85.85%\n",
            "Epoch [172/200], Loss: 0.2242, Test Acc: 85.98%\n",
            "Epoch [173/200], Loss: 0.2202, Test Acc: 85.95%\n",
            "Epoch [174/200], Loss: 0.2197, Test Acc: 86.29%\n",
            "Epoch [175/200], Loss: 0.2206, Test Acc: 86.28%\n",
            "Epoch [176/200], Loss: 0.2180, Test Acc: 86.18%\n",
            "Epoch [177/200], Loss: 0.2186, Test Acc: 86.22%\n",
            "Epoch [178/200], Loss: 0.2187, Test Acc: 86.26%\n",
            "Epoch [179/200], Loss: 0.2135, Test Acc: 86.18%\n",
            "Epoch [180/200], Loss: 0.2147, Test Acc: 86.17%\n",
            "Epoch [181/200], Loss: 0.2158, Test Acc: 86.65%\n",
            "Epoch [182/200], Loss: 0.2163, Test Acc: 85.69%\n",
            "Epoch [183/200], Loss: 0.2146, Test Acc: 86.28%\n",
            "Epoch [184/200], Loss: 0.2103, Test Acc: 86.43%\n",
            "Epoch [185/200], Loss: 0.2082, Test Acc: 85.48%\n",
            "Epoch [186/200], Loss: 0.2120, Test Acc: 86.03%\n",
            "Epoch [187/200], Loss: 0.2099, Test Acc: 86.32%\n",
            "Epoch [188/200], Loss: 0.2073, Test Acc: 86.13%\n",
            "Epoch [189/200], Loss: 0.2100, Test Acc: 86.56%\n",
            "Epoch [190/200], Loss: 0.2088, Test Acc: 86.39%\n",
            "Epoch [191/200], Loss: 0.2053, Test Acc: 86.38%\n",
            "Epoch [192/200], Loss: 0.2026, Test Acc: 86.39%\n",
            "Epoch [193/200], Loss: 0.2037, Test Acc: 86.73%\n",
            "Epoch [194/200], Loss: 0.2031, Test Acc: 85.86%\n",
            "Epoch [195/200], Loss: 0.2022, Test Acc: 86.10%\n",
            "Epoch [196/200], Loss: 0.2039, Test Acc: 85.83%\n",
            "Epoch [197/200], Loss: 0.2004, Test Acc: 85.28%\n",
            "Epoch [198/200], Loss: 0.2024, Test Acc: 86.42%\n",
            "Epoch [199/200], Loss: 0.1994, Test Acc: 86.09%\n",
            "Epoch [200/200], Loss: 0.2043, Test Acc: 86.10%\n",
            "ResNet-10 Training time: 2454.64s after 200 epochs\n"
          ]
        }
      ]
    }
  ]
}